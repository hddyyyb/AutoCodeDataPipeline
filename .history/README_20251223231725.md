# AutoCodeDataPipeline
**Repository-Grounded Training Data Generation for Instruction-Tuned LLMs**


## 1. Project Motivation

This project directly addresses the interview task:

> **“Design a system that automatically generates high-quality training data from a local code repository, enabling a fine-tuned model to answer repository-specific business logic questions and propose architecture-level design solutions.”**

The system is designed for **Qwen 2.5 series models**, but is model-agnostic.

---

## 2. Target Scenarios

### Scenario 1: Repository-Grounded QA Generation

Automatically generate question–answer pairs from a local code repository.

Each QA sample includes:
- Original code snippets as evidence
- Stable code references (chunk_id)
- A structured and auditable reasoning trace

---

### Scenario 2: Repository-Constrained Design Generation

Given a new requirement, the system generates:
- A design proposal aligned with the existing repository architecture
- Explicit architectural assumptions
- Referenced evidence code
- A reasoning trace explaining design decisions

---

## 3. Core Design Principles

| Principle | Description |
| --------- | ----------- |
| Traceability | Every sample is grounded in source code via stable chunk_id |
| Auditability | Reasoning traces are structured and verifiable |
| Automation | No manual annotation; all data is pipeline-generated |
| Extensibility | Supports new repositories, domains, and task types |
| Training–Inference Alignment | Same constraints apply in training and inference |

---

## 4. End-to-End Pipeline

Repository  
↓  
Chunk Index (Step01)  
↓  
Domain Map (Step02)  
↓  
Rules & Flows Extraction (Step03)  
↓  
QA / Design Sample Generation (Step04 / Step05)  
↓  
Validation & Deduplication (Step06)  
↓  
SFT Dataset Preparation (Step08)  
↓  
Optional LoRA Fine-tuning (Step09)  
↓  
Strict Inference with Validation (Step10)

Each step is independently auditable and reproducible.

---

## 5. Repository Used

Public repository: https://github.com/macrozheng/mall  
Selected domains: Order and Inventory

Rationale:
- Rich business logic
- Clear architectural layering (Controller / Service / Mapper)
- Real-world concurrency and consistency rules

---

## 6. Dataset Characteristics

- Two task types: `qa` and `design`
- Unified schema with grounding metadata
- Explicit evidence code snippets
- Structured reasoning traces (non-CoT)

Both tasks are directly usable for instruction fine-tuning.

---

## 7. How to Run the Pipeline

Minimal execution sequence:

```bash
python scripts/01_index_repo.py  
python scripts/02_build_domain_map.py  
python scripts/03_extract_rules_and_flows.py  
python scripts/04_generate_qa.py  
python scripts/05_generate_design_tasks.py  
python scripts/06_postprocess_and_validate.py  
python scripts/08_prepare_sft_data.py  
```
Optional validation:
```bash
python scripts/09_lora_finetune.py  
python scripts/10_infer_lora.py  
```

---

## 8. Optional Fine-Tuning Demo

Base model: Qwen2.5-0.5B-Instruct  
Method: LoRA fine-tuning  

Purpose:
- Verify dataset usability
- Validate training and inference compatibility

This step is optional and provided as an additional validation.

---

## 9. Why This Project Meets the Interview Requirements

- Covers both required scenarios
- Fully automated training data generation
- Explicit code grounding and reasoning traces
- Architecture-aware design generation
- Ready for real fine-tuning and inference

---

## 10. Documentation Overview

Detailed design documents are available in the `docs/` directory:

- `00_design_document.md` – System design rationale
- `01_dataset_schema.md` – Dataset schema definition
- `02_domain_map.md` – Domain map construction
- `03_pipeline_design.md` – Pipeline and governance design
- `04_demo_and_results.md` – Dataset statistics and results
