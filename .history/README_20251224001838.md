# AutoCodeDataPipeline

AutoCodeDataPipeline is an end-to-end system for **automatically generating high-quality, repository-grounded training data** from a local codebase.

This project is implemented as a direct response to the interview assignment:

> **Local Code Repository–Based Intelligent Training Data Generation and Processing**

The system is designed to support **instruction fine-tuning of Qwen 2.5–series models**, enabling them to:
- Answer questions about repository-specific business rules and execution flows
- Produce architecture-level design proposals grounded in existing code

---

## 1. Target Repository

- Repository: https://github.com/macrozheng/mall
- Scope: Order & Inventory domains
- Language: Java (Spring-based backend)

The repository is treated as a **local codebase** and indexed without modifying its original source code.

---

## 2. Supported Scenarios (Aligned with Assignment)

### Scenario 1: Repository-Grounded QA Generation

Automatically generate question–answer pairs based on:
- Business rules
- Execution flows inferred from the code repository

Each QA sample includes:
- Explicit code evidence (original source snippets)
- Structured reasoning trace (auditable, non-CoT)

---

### Scenario 2: Repository-Constrained Design Tasks

Generate architecture and design proposals for new requirements, constrained by:
- Existing layered architecture (Controller / Service / Mapper)
- Existing business rules and execution flows

Each design sample includes:
- Evidence code snippets from the repository
- A reasoning trace explaining design decisions

---

## 3. Core Design Principles

1. **Repository-Grounded Only**  
   All generated samples must be traceable to original code via stable `chunk_id`s.

2. **Auditable Reasoning (Non-CoT)**  
   Reasoning traces are short, structured, and verifiable.  
   Free-form chain-of-thought is explicitly avoided.

3. **Training–Inference Alignment**  
   The same strict formats and constraints are enforced during:
   - Dataset generation
   - Model fine-tuning
   - Inference-time validation

---

## 4. End-to-End Pipeline Overview

```
Code Repository
↓
Repository Indexing
↓
Domain & Structural Inference
↓
Rule & Flow Abstraction
↓
QA / Design Task Generation
↓
Postprocess & Validation
↓
Metrics & Reports
↓
(Optional) Model Fine-tuning & Inference
```

---

## 5. Pipeline Steps

| Step | Script | Purpose |
|-----:|--------|---------|
| 01 | `scripts/01_index_repo.py` | Chunk-level repository indexing |
| 02 | `scripts/02_build_domain_map.py` | Infer domains, entities, operations, and candidate flows |
| 03 | `scripts/03_extract_rules_and_flows.py` | Abstract business rules and execution flows |
| 04 | `scripts/04_generate_qa.py` | Generate QA samples with evidence and reasoning traces |
| 05 | `scripts/05_generate_design_tasks.py` | Generate repository-constrained design tasks |
| 06 | `scripts/06_postprocess_and_validate.py` | Validate, deduplicate, and split datasets |
| 07 | `scripts/07_metrics_report.py` | Generate metrics and quality reports |
| 08 | `scripts/08_prepare_sft_data.py` | Prepare instruction-tuning datasets (optional) |
| 09 | `scripts/09_lora_finetune.py` | Lightweight LoRA fine-tuning (optional) |
| 10 | `scripts/10_infer_lora.py` | Inference-time validation (optional) |


---

## 6. Pipeline Artifacts (High-Level)


Each pipeline step produces explicit, versioned artifacts under the `data/` directory.

| Stage | Output Artifact | Description |
|------:|-----------------|-------------|
| 01 | `data/raw_index/repo_index.jsonl` | Chunk-level index of source code with traceable IDs |
| 02 | `data/extracted/domain_map.json` | Domain map with entities, operations, and evidence |
| 03 | `data/extracted/rules_and_flows.json` | Abstracted business rules and execution flows |
| 04 | `data/samples/qa_raw.jsonl` | Raw QA samples with evidence and reasoning traces |
| 05 | `data/samples/design_tasks.jsonl` | Repository-constrained design tasks |
| 06 | `data/dataset/final_{train,dev,test}.jsonl` | Validated, deduplicated, grounded dataset (text + meta) |
| 07 | `docs/04_demo_and_results.md` | Human-readable metrics and quality report |
| 08 | `data/sft/*.jsonl` | Instruction-tuning datasets (light / grounded) |


Intermediate artifacts are intentionally preserved to ensure **full auditability** and **incremental recomputation**.

---

## 7. How to Run (Minimal Example)

### Step 1–3: Build Structured Semantic Artifacts

```bash
python scripts/01_index_repo.py
python scripts/02_build_domain_map.py
python scripts/03_extract_rules_and_flows.py
```

### Step 4–6: Generate and Validate Datasets
```bash
python scripts/04_generate_qa.py
python scripts/05_generate_design_tasks.py
python scripts/06_postprocess_and_validate.py
```
### Step 7: Generate Metrics Report
```bash
python scripts/07_metrics_report.py
```
At this point, the repository-grounded dataset and evaluation reports have been fully generated.

### (Optional) Prepare Instruction-Tuning Data

If instruction-tuning is desired, the validated dataset can be converted into a supervised fine-tuning (SFT) format:

```
python scripts/08_prepare_sft_data.py
```

This step does not modify the original dataset and only produces model-consumable training files.

### (Optional) LoRA Fine-tuning and Inference

For end-to-end validation, lightweight LoRA fine-tuning and inference can be performed:

```
python scripts/09_lora_finetune.py
python scripts/10_infer_lora.py
```

These steps are optional and serve as a sanity check to demonstrate that the generated data is directly usable for downstream model adaptation.

---

## 8. Documentation Guide

All design documents are located under the `docs\` directory.

| Document | Description |
|---------|-------------|
| `docs/00_design_document.md` | Design goals and requirement traceability |
| `docs/01_dataset_schema.md` | Dataset schema and metadata definition |
| `docs/02_domain_map.md` | Domain map construction and representativeness |
| `docs/03_pipeline_design.md` | Authoritative end-to-end pipeline design |
| `docs/04_demo_and_results.md` | Dataset statistics and demo results |

---

## 9. Notes

- Multilingual support is implemented at the task generation layer and is optional.
- All generated samples include explicit code evidence and reasoning traces.
- The pipeline is designed to be extensible to new repositories, domains, and task types.
