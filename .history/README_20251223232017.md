# AutoCodeDataPipeline

AutoCodeDataPipeline is an end-to-end engineering pipeline for generating **auditable, repository-grounded training data** from a local codebase.

The project is implemented as a response to the interview assignment:

> **Local Code Repository–Based Intelligent Training Data Generation and Processing**

The pipeline is designed not only to generate QA and design samples, but also to **prove that these samples are directly consumable by LLMs under strict inference constraints**.

---

## Target Repository

- Repository: https://github.com/macrozheng/mall
- Scope: Order & Inventory domains
- Language: Java (Spring-based backend)

The repository is treated as a local codebase and indexed without modifying the original source.

---

## Design Principles

1. **Repository-grounded only**  
   All generated samples must be traceable to original source code chunks (`chunk_id`).

2. **Auditable reasoning (non-CoT)**  
   Reasoning traces are structured, bounded, and verifiable.  
   Free-form chain-of-thought is explicitly avoided.

3. **Training–Inference consistency**  
   The SFT data format is strictly aligned with inference-time hard checks, ensuring no format drift between training and deployment.

---

## Supported Scenarios

1. **Scenario 1: QA Generation**  
   Generate question–answer pairs based on business rules and execution flows, with:
   - Explicit code evidence
   - Structured reasoning traces
   - Strict traceability guarantees

2. **Scenario 2: Architecture & Design Tasks**  
   Generate repository-constrained design tasks that:
   - Respect existing layered architecture
   - Include code-grounded evidence
   - Provide auditable design reasoning

---

## End-to-End Pipeline Overview



```
Code Repository
↓
Repository Indexing
↓
Domain & Structural Inference
↓
Rule & Flow Abstraction
↓
QA / Design Task Generation
↓
Postprocess & Validation
↓
Metrics & Reports
↓
(Optional) Model Fine-tuning & Inference
```


Each stage produces explicit intermediate artifacts, ensuring full traceability from generated samples back to source code.

---

## 2. Pipeline Execution Steps

The pipeline is implemented as a sequence of Python scripts.
Each script corresponds to a well-defined pipeline stage.

| Step | Script | Purpose |
|-----:|--------|---------|
| 01 | `scripts/01_index_repo.py` | Index and chunk the target repository |
| 02 | `scripts/02_build_domain_map.py` | Infer domains, entities, operations, and candidate flows |
| 03 | `scripts/03_extract_rules_and_flows.py` | Abstract business rules and execution flows |
| 04 | `scripts/04_generate_qa.py` | Generate QA samples with evidence and reasoning traces |
| 05 | `scripts/05_generate_design_tasks.py` | Generate architecture/design tasks |
| 06 | `scripts/06_postprocess_and_validate.py` | Validate, deduplicate, and compute statistics |
| 07 | `scripts/07_metrics_report.py` | Generate human-readable metrics and reports |
| 08 | `scripts/08_prepare_sft_data.py` | Prepare instruction-tuning data, Step08 consumes only the validated final dataset and never operates on raw samples. (optional) |
| 09 | `scripts/09_lora_finetune.py` | Lightweight LoRA fine-tuning (optional) |
| 10 | `scripts/10_infer_lora.py` | Inference and quick validation (optional) |

You may stop the pipeline at any intermediate stage depending on your goal (e.g., dataset generation only vs. end-to-end validation).

The execution steps and their resulting artifacts are intentionally documented separately to distinguish **processing logic** from **persistent data outputs**.


#### Pipeline Artifacts (High-Level)

Each pipeline step produces explicit, versioned artifacts under the `data/` directory:

| Stage | Output Artifact | Description |
|------|-----------------|-------------|
| 01 | `data/raw_index/repo_index.jsonl` | Chunk-level index of source code with traceable IDs |
| 02 | `data/extracted/domain_map.json` | Domain map with entities, operations, and evidence |
| 03 | `data/extracted/rules_and_flows.json` | Abstracted business rules and execution flows |
| 04 | `data/samples/qa_raw.jsonl` | Raw QA samples with evidence and reasoning traces |
| 05 | `data/samples/design_tasks.jsonl` | Repository-constrained design tasks |
| 06 | `data/dataset/final_{train,dev,test}.jsonl` | Validated, deduplicated, grounded dataset (text + meta) |
| 07 | `docs/04_demo_and_results.md` | Human-readable metrics and quality report |
| 08 | `data/sft/*.jsonl` | Instruction-tuning datasets (light / grounded) |


Intermediate artifacts are intentionally preserved to ensure full auditability and incremental recomputation.

---

## 3. How to Run (Minimal Example)

```bash
# Step 1–3: build structured semantic artifacts
python scripts/01_index_repo.py
python scripts/02_build_domain_map.py
python scripts/03_extract_rules_and_flows.py

# Step 4–6: generate and validate datasets
python scripts/04_generate_qa.py
python scripts/05_generate_design_tasks.py
python scripts/06_postprocess_and_validate.py

# Step 7: generate metrics report
python scripts/07_metrics_report.py

```
At this point, the repository-grounded dataset and evaluation reports have been fully generated.

#### 3.1 (Optional) Prepare Instruction-Tuning Data

If instruction-tuning is desired, the validated dataset can be converted into a supervised fine-tuning (SFT) format:

```
python scripts/08_prepare_sft_data.py
```

This step does not modify the original dataset and only produces model-consumable training files.

#### 3.2 (Optional) LoRA Fine-tuning and Inference

For end-to-end validation, lightweight LoRA fine-tuning and inference can be performed:

```
python scripts/09_lora_finetune.py
python scripts/10_infer_lora.py
```

These steps are optional and serve as a sanity check to demonstrate that the generated data is directly usable for downstream model adaptation.

---

## 4. Documentation Guide

All design documents are located under the docs directory.

| Document | Description |
|---------|-------------|
| docs/00_design_document.md | Design overview and requirement traceability |
| docs/01_dataset_schema.md | Dataset schema and data contract |
| docs/02_domain_map.md | Domain map construction and representativeness guarantees |
| docs/03_pipeline_design.md | Authoritative end-to-end pipeline design |
| docs/04_demo_and_results.md | Metrics, statistics, and demo results |

For a complete understanding of the system architecture and design decisions, start with docs/03_pipeline_design.md.

---

## 5. Notes

- Multilingual support is implemented at the task generation layer and is optional.
- All generated samples include explicit code evidence and reasoning traces.
- The pipeline is designed to be extensible to new repositories, domains, and task types.
