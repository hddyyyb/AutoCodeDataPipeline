# AutoCodeDataPipeline

AutoCodeDataPipeline is an end-to-end engineering pipeline for generating
auditable, repository-grounded training data from a local codebase.
It is designed to support domain-specific LLM fine-tuning and evaluation.

This project is implemented as a response to the interview assignment:
“Local Code Repository–Based Intelligent Training Data Generation and Processing”.

#### Target Repository

This pipeline is evaluated on the following repository:

- Repository: https://github.com/macrozheng/mall
- Scope: Order & Inventory domains
- Language: Java (Spring-based backend)

The repository is treated as a local codebase and indexed via configurable
scope rules without modifying pipeline code.

#### Supported Scenarios
The pipeline targets two scenarios:
1. QA generation based on business rules and execution flows extracted from a code repository.
2. Architecture and design task generation constrained by the existing repository structure.

---

## 1. End-to-End Pipeline Overview

The pipeline transforms a local code repository into structured datasets
(and optionally validates them via lightweight model adaptation).

```
Code Repository
↓
Repository Indexing
↓
Domain & Structural Inference
↓
Rule & Flow Abstraction
↓
QA / Design Task Generation
↓
Postprocess & Validation
↓
Metrics & Reports
↓
(Optional) Model Fine-tuning & Inference
```


Each stage produces explicit intermediate artifacts, ensuring full
traceability from generated samples back to source code.

---

## 2. Pipeline Execution Steps

The pipeline is implemented as a sequence of Python scripts.
Each script corresponds to a well-defined pipeline stage.

| Step | Script | Purpose |
|-----:|--------|---------|
| 01 | `scripts/01_index_repo.py` | Index and chunk the target repository |
| 02 | `scripts/02_build_domain_map.py` | Infer domains, entities, operations, and candidate flows |
| 03 | `scripts/03_extract_rules_and_flows.py` | Abstract business rules and execution flows |
| 04 | `scripts/04_generate_qa.py` | Generate QA samples with evidence and reasoning traces |
| 05 | `scripts/05_generate_design_tasks.py` | Generate architecture/design tasks |
| 06 | `scripts/06_postprocess_and_validate.py` | Validate, deduplicate, and compute statistics |
| 07 | `scripts/07_metrics_report.py` | Generate human-readable metrics and reports |
| 08 | `scripts/08_prepare_sft_data.py` | Prepare instruction-tuning data (optional) |
| 09 | `scripts/09_lora_finetune.py` | Lightweight LoRA fine-tuning (optional) |
| 10 | `scripts/10_infer_lora.py` | Inference and quick validation (optional) |

You may stop the pipeline at any intermediate stage depending on your goal
(e.g., dataset generation only vs. end-to-end validation).

The execution steps and their resulting artifacts are intentionally documented
separately to distinguish **processing logic** from **persistent data outputs**.


#### Pipeline Artifacts (High-Level)

Each pipeline step produces explicit, versioned artifacts under the `data/` directory:

| Stage | Output Artifact | Description |
|------|-----------------|-------------|
| 01 | `data/raw_index/repo_index.jsonl` | Chunk-level index of source code with traceable IDs |
| 02 | `data/extracted/domain_map.json` | Domain map with entities, operations, and evidence |
| 03 | `data/extracted/rules_and_flows.json` | Abstracted business rules and execution flows |
| 04 | `data/samples/qa_raw.jsonl` | Raw QA samples with evidence and reasoning traces |
| 05 | `data/samples/design_tasks.jsonl` | Repository-constrained design tasks |
| 06 | `data/processed/dataset_final.jsonl` | Validated and deduplicated dataset |
| 07 | `reports/metrics.md` | Dataset statistics and coverage metrics |

Intermediate artifacts are intentionally preserved to ensure full auditability
and incremental recomputation.

---

## 3. How to Run (Minimal Example)

```bash
# Step 1–3: build structured semantic artifacts
python scripts/01_index_repo.py
python scripts/02_build_domain_map.py
python scripts/03_extract_rules_and_flows.py

# Step 4–6: generate and validate datasets
python scripts/04_generate_qa.py
python scripts/05_generate_design_tasks.py
python scripts/06_postprocess_and_validate.py

# Step 7: generate metrics report
python scripts/07_metrics_report.py

```
At this point, the repository-grounded dataset and evaluation reports
have been fully generated.






---

## 4. Documentation Guide

All design documents are located under the docs directory.

| Document | Description |
|---------|-------------|
| docs/00_design_document.md | Design overview and requirement traceability |
| docs/01_dataset_schema.md | Dataset schema and data contract |
| docs/02_domain_map.md | Domain map construction and representativeness guarantees |
| docs/03_pipeline_design.md | Authoritative end-to-end pipeline design |
| docs/04_demo_and_results.md | Metrics, statistics, and demo results |

For a complete understanding of the system architecture and design decisions,
start with docs/03_pipeline_design.md.

---

## 5. Notes

- Multilingual support is implemented at the task generation layer and is optional.
- All generated samples include explicit code evidence and reasoning traces.
- The pipeline is designed to be extensible to new repositories, domains, and task types.
