# End-to-End Pipeline Design

This document describes the **authoritative end-to-end design**
of AutoCodeDataPipeline.

The pipeline is designed to transform a local code repository into
**auditable, repository-grounded training data**
that remains valid under downstream fine-tuning and inference constraints.

---

## 1. Design Goals

The pipeline is designed with the following goals:

- **Traceability**: every generated sample must be traceable to source code
- **Auditability**: reasoning must be explicit, structured, and verifiable
- **Training–Inference Consistency**: data formats and constraints must align across stages
- **Operational Safety**: invalid or hallucinated outputs must be detectable and correctable
- **Extensibility**: new repositories, domains, and task types can be added incrementally

---

## 2. End-to-End Pipeline Overview

At a high level, the pipeline consists of three conceptual phases:

1. **Structural and Semantic Modeling (Step01–Step03)**  
   Raw source code is indexed and transformed into structured semantic artifacts,
   including architectural boundaries, domain entities, operations, rules, and flows.

2. **Dataset Generation and Governance (Step04–Step07)**  
   Repository-grounded QA and design samples are generated, validated,
   deduplicated, and evaluated for quality and coverage.

3. **Training and Inference Alignment (Optional Step08–Step10)**  
   Validated samples are converted into SFT-ready data,
   and strict inference-time validation is applied to ensure deployment safety.

Each phase produces explicit intermediate artifacts,
ensuring full auditability and incremental recomputation.

---

## 3. Step04–Step06: Dual-Layer Sample Representation

During QA and design generation, samples may contain two layers of representation:

- **Legacy semantic fields**  
  (`question` / `answer`, `requirement` / `design_output`)
- **Training-ready fields**  
  (`text`, `meta_v2`)

This dual-layer structure serves two purposes:
- Preserving human-readable semantics for inspection and debugging
- Providing model-consumable instruction data without information leakage

The `meta_v2` field is treated as the **authoritative grounding layer**,
containing evidence references and reasoning traces.

---

## 4. Step06: Validation and Governance

Before samples enter training or inference stages,
strict validation and governance rules are applied.

Validation enforces:
- Schema completeness
- Validity of referenced `chunk_id`s
- Mandatory inclusion of original code snippets
- Minimum reasoning depth
- Deduplication based on semantic keys

Only samples that pass all validation checks
are promoted to the final dataset.

This step ensures that dataset quality is enforced **by construction**,
rather than relying on downstream filtering.

---

## 5. Step08: SFT Preparation

Validated samples are converted into SFT-ready formats
without modifying the underlying grounding information.

Key properties include:
- Optional language splitting (zh / en)
- Lightweight or grounded instruction modes
- Preservation of metadata for auditability

This separation ensures that training data
remains consistent with inference-time constraints.

---

## 6. Step10: Strict Inference Validation and Repair

Inference is performed under **hard operational constraints**:

- Outputs must follow a fixed template
- JSON output is explicitly forbidden
- Referenced `chunk_id`s must exist in an evidence whitelist
- Evidence paths are validated against the repository index
- Reasoning traces must meet minimum depth requirements

If a model output violates any constraint,
a **forced repair mechanism** reconstructs a compliant answer
using only whitelisted evidence.

This guarantees that the system is not only trained on high-quality data,
but is also **operationally safe for deployment**.

---

## 7. Summary

This pipeline design establishes a strict and auditable contract between:
- Repository source code
- Automatically generated training data
- Downstream fine-tuning and inference systems

By enforcing traceability, validation, and alignment at every stage,
AutoCodeDataPipeline ensures that generated data is
reliable, explainable, and directly usable in real-world settings.
