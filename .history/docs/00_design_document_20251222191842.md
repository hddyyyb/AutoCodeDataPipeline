# AutoCodeDataPipeline – Design Overview

## 1. Purpose of This Document

This document provides a **high-level overview and navigation entry point**
for the AutoCodeDataPipeline project.

It is intended to help readers quickly understand:
- the overall goals of the project,
- the types of datasets generated,
- the structure of the documentation,
- and where to find authoritative design details.

⚠️ **This document is not the authoritative system design specification.**  
The complete end-to-end pipeline design is documented in:

- **[docs/03_pipeline_design.md](03_pipeline_design.md)**

---

## 2. Project Overview

AutoCodeDataPipeline is an automated system for generating **auditable,
repository-grounded training data** from a local code repository, with
optional downstream validation via lightweight model adaptation.

The pipeline targets two task scenarios required by the assignment:

- **Scenario 1: QA generation** grounded in code-level business rules and execution flows  
- **Scenario 2: Architecture/design generation** constrained by the existing
  repository structure (e.g., Controller / Service / Mapper layering)

The system is designed with explicit traceability, controllable coverage,
and extensibility as first-class goals.

---

## 3. Dataset Design

The generated dataset consists of two task types:

- **QA**
- **Design**

Each sample explicitly records:
- task type,
- natural language (`language`),
- source code evidence (`chunk_id`s),
- and structured reasoning traces.

The full dataset schema, field definitions, and examples are specified in:

- **[docs/01_dataset_schema.md](01_dataset_schema.md)**

Representative samples are available in:

- `data/samples/qa_samples.jsonl`
- `data/samples/design_samples.jsonl`

---

## 4. Automated Data Generation Pipeline (Summary)

The dataset is produced through a multi-stage automated pipeline, including:

- Repository indexing and chunking
- Domain and structural inference
- Rule and flow abstraction
- QA and design task generation
- Post-processing, validation, and deduplication
- Optional model fine-tuning and inference

⚠️ **Detailed module responsibilities, data flow, and design rationale are
intentionally omitted here** and are fully described in:

- **[docs/03_pipeline_design.md](03_pipeline_design.md)**

---

## 5. Quality, Coverage, and Correctness Guarantees

To ensure data correctness and stable quality, the pipeline enforces:

- Schema validation
- Evidence grounding against indexed code chunks
- Trace-based reasoning requirements
- Deduplication and diversity control across:
  - domains,
  - task types,
  - difficulty levels,
  - and natural languages

Empirical statistics and coverage reports are provided in:

- **[docs/04_demo_and_results.md](04_demo_and_results.md)**

---

## 6. Extensibility and Future Work

The system is designed to support future requirement changes:

- New repositories can be indexed without code changes
- New task types can be added via modular generators
- Multilingual support is implemented at the task template layer
- Lightweight LoRA fine-tuning enables rapid validation
- Incremental updates can be supported by re-running selective pipeline stages

This design allows the pipeline to evolve while maintaining data consistency
and auditability.

---

## 7. Document Map

For clarity, the documentation is organized as follows:

- **[docs/00_design_document.md](00_design_document.md)**  
  High-level overview and requirement traceability (this document)

- **[docs/01_dataset_schema.md](01_dataset_schema.md)**  
  Dataset schema and data contract

- **[docs/02_domain_map.md](02_domain_map.md)**  
  Domain map construction and representativeness guarantees

- **[docs/03_pipeline_design.md](03_pipeline_design.md)**  
  **Authoritative end-to-end pipeline design**

- **[docs/04_demo_and_results.md](04_demo_and_results.md)**  
  Metrics, statistics, and empirical results

---


## 8. Requirement Traceability (Assignment Checklist)

This section maps the assignment requirements and evaluation criteria to the
exact documentation locations in this repository.

### 8.1 Assignment Requirements

1. **Design a dataset schema for QA and Design tasks, including metadata, and
   explain how diversity and representativeness are ensured**
   - Dataset schema, evidence format, and reasoning trace constraints:  
     → **[docs/01_dataset_schema.md](01_dataset_schema.md)**
   - Structural coverage across layers, domains, and execution scenarios:  
     → **[docs/02_domain_map.md](02_domain_map.md)**

2. **Use a public GitHub repository for dataset generation and testing**
   - Repository scope and end-to-end pipeline definition:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)**
   - Dataset statistics and empirical validation:  
     → **[docs/04_demo_and_results.md](04_demo_and_results.md)**

3. **Multilingual support (optional)**
   - Language field and schema extensibility:  
     → **[docs/01_dataset_schema.md](01_dataset_schema.md)**
   - Task-level multilingual generation and templates:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)** (Section 4.4.3)
   - Language distribution in generated samples:  
     → **[docs/04_demo_and_results.md](04_demo_and_results.md)**

4. **Lightweight fine-tuning for quick validation (optional)**
   - Model adaptation and inference loop:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)** (Section 4.7)

---

### 8.2 Evaluation Criteria Mapping

1. **Scenario coverage and logical correctness**
   - QA + Design task structure and required fields:  
     → **[docs/01_dataset_schema.md](01_dataset_schema.md)**
   - Rule/flow abstraction and business-process grounding:  
     → **[docs/02_domain_map.md](02_domain_map.md)**

2. **Effectiveness and innovation of data processing**
   - Layered pipeline design and automation strategy:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)**

3. **System architecture completeness and extensibility**
   - End-to-end pipeline and modular layering:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)**
   - Explicit extension points and future evolution:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)** (Section 7)

4. **Clarity and compliance of sample data and reasoning traces**
   - Evidence grounding and trace constraints:  
     → **[docs/01_dataset_schema.md](01_dataset_schema.md)**
   - Validation, deduplication, and quality checks:  
     → **[docs/03_pipeline_design.md](03_pipeline_design.md)** (Section 4.5)
   - Empirical quality statistics:  
     → **[docs/04_demo_and_results.md](04_demo_and_results.md)**