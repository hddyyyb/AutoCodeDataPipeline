# End-to-End Pipeline Design

This document describes the authoritative design of AutoCodeDataPipeline.

---

## Step04â€“Step06: Dual-Layer Sample Representation

Generated samples may contain:
- Legacy fields (question/answer, requirement/design_output)
- Training-ready fields (text, meta_v2)

This dual structure ensures:
- Backward compatibility
- Direct usability for SFT and inference

`meta_v2` is treated as the authoritative grounding layer.

---

## Step06: Validation and Governance

Validation enforces:
- Schema completeness
- Evidence chunk validity
- Mandatory original code snippets
- Minimum reasoning depth
- Deduplication by semantic keys

Only validated samples are allowed to enter training or inference stages.

---

## Step08: SFT Preparation

Validated samples are converted into SFT-ready format.

Key properties:
- Optional language splitting (zh / en)
- Lightweight or grounded modes
- Meta preserved for auditability

---

## Step10: Strict Inference Validation

Inference is performed under **hard constraints**:

- Output must follow a fixed template
- JSON output is forbidden
- `chunk_id` must be 16-hex and from an evidence whitelist
- Evidence references are path-validated
- Reasoning steps must meet minimum depth

If the model output violates constraints, a **forced repair mechanism** reconstructs a compliant answer from whitelisted evidence.

This step demonstrates that the dataset is not only high-quality, but also **operationally safe for deployment**.

