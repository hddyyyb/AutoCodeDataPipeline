# AutoCodeDataPipeline – Design Overview

## 1. Purpose of This Document

This document provides a **high-level overview and navigation entry point**
for the AutoCodeDataPipeline project.

It is intended to help readers quickly understand:
- the overall goals of the project,
- the types of datasets generated,
- the structure of the documentation,
- and where to find authoritative design details.

⚠️ **This document is not the authoritative system design specification.**  
The complete end-to-end pipeline design is documented in:

- **docs/03_pipeline_design.md**

---

## 2. Project Overview

AutoCodeDataPipeline is an automated system for generating **auditable,
repository-grounded training data** from a local code repository, with
optional downstream validation via lightweight model adaptation.

The pipeline targets two primary task categories:

- **QA tasks** grounded in code-level business rules and execution flows
- **Architecture and design tasks** constrained by the existing repository
  structure (e.g., Controller / Service / Mapper layering)

The system is designed with explicit traceability, controllable coverage,
and extensibility as first-class goals.

---

## 3. Dataset Design

The generated dataset consists of two task types:

- **QA**
- **Design**

Each sample explicitly records:
- task type,
- natural language (`language`),
- source code evidence (`chunk_id`s),
- and structured reasoning traces.

The full dataset schema, field definitions, and examples are specified in:

- **docs/01_dataset_schema.md**

Representative samples are available in:

- `data/samples/qa_samples.jsonl`
- `data/samples/design_samples.jsonl`

---

## 4. Automated Data Generation Pipeline (Summary)

The dataset is produced through a multi-stage automated pipeline, including:

- Repository indexing and chunking
- Domain and structural inference
- Rule and flow abstraction
- QA and design task generation
- Post-processing, validation, and deduplication
- Optional model fine-tuning and inference

⚠️ **Detailed module responsibilities, data flow, and design rationale are
intentionally omitted here** and are fully described in:

- **docs/03_pipeline_design.md**

---

## 5. Quality, Coverage, and Correctness Guarantees

To ensure data correctness and stability, the pipeline enforces:

- Schema validation
- Evidence grounding against indexed code chunks
- Trace-based reasoning requirements
- Deduplication and diversity control across:
  - domains,
  - task types,
  - difficulty levels,
  - and natural languages

Empirical statistics and coverage reports are provided in:

- **docs/05_demo_and_results.md**

---

## 6. Extensibility and Future Work

The system is designed to be extensible:

- New repositories can be indexed without code changes
- New task types can be added via modular generators
- Multilingual support is implemented at the task template layer
- Lightweight LoRA fine-tuning enables rapid validation
- Incremental updates can be supported by re-running selective pipeline stages

This design allows the pipeline to evolve with future requirements while
maintaining data consistency and auditability.

---

## 7. Document Map

For clarity, the documentation is organized as follows:

- **docs/00_design_document.md**  
  High-level overview and navigation (this document)

- **docs/01_dataset_schema.md**  
  Dataset schema and data contract

- **docs/02_domain_map.md**  
  Domain map construction and representativeness guarantees

- **docs/03_pipeline_design.md**  
  **Authoritative end-to-end pipeline design**

- **docs/05_demo_and_results.md**  
  Metrics, statistics, and empirical results
