# Pipeline Design: End-to-End Repository-Grounded Training Data Generation and Model Adaptation

## 1. Problem Definition and Objectives

This project aims to design an end-to-end engineering pipeline that transforms a local enterprise code repository into **auditable, task-oriented training data**, and further validates the effectiveness of such data through lightweight model adaptation.

Unlike generic code QA or RAG-style systems, this pipeline focuses on:

- business process and rule understanding rather than syntax-level explanation,
- explicit grounding in source code evidence,
- controllable coverage over domains, architectural layers, and execution scenarios,
- reproducibility and auditability at every stage.

The target repository in this project is an enterprise Java system (`macrozheng/mall`), with the scope restricted to **order** and **stock** domains.

---

## 2. Overall Pipeline Architecture

The pipeline forms a closed loop from **code → data → model → inference**, rather than stopping at data generation.
```
Local Code Repository
↓
Repository Indexing
↓
repo_index.jsonl
↓
Domain & Structure Inference
↓
domain_map.json
↓
Rule & Flow Abstraction
↓
rules.jsonl / flows.jsonl
↓
Task-Oriented Sample Generation
↓
QA / Design Datasets
↓
Postprocess & Validation
↓
Metrics & Reports
↓
SFT Preparation
↓
LoRA Fine-tuning
↓
Inference & Demo
```

Each stage reduces ambiguity and increases semantic structure, while preserving traceability back to the original code.

---

## 3. Core Design Principles

1. **Traceability over perfect semantic accuracy**  
   All generated artifacts must reference concrete source code chunks (`chunk_id`s), even if the inferred semantics are heuristic.

2. **Structure before sampling**  
   Raw code chunks are never sampled directly. Intermediate semantic structures are always constructed first.

3. **Heuristics over heavyweight static analysis**  
   The pipeline avoids full AST parsing and call graph reconstruction in favor of lightweight, explainable heuristics.

4. **Domain-aware, not file-centric**  
   Grouping and sampling are driven by business concepts (entities, operations, flows), not by file boundaries.

5. **End-to-end validation**  
   The usefulness of generated data is verified through actual model adaptation and inference.

---

## 4. Layered Pipeline Design

### 4.1 Repository Indexing Layer

**Corresponding script:**  
- `01_index_repo.py`

**Purpose**

This layer converts an unstructured code repository into a **chunk-addressable index**, which serves as the traceability root of the entire pipeline.

**Key Design Choices**

- Code is split into stable chunks with deterministic `chunk_id`s derived from file path, line range, and content hash.
- Each chunk records:
  - `file_path`
  - `start_line` / `end_line`
  - `content`
  - `lang`
  - `symbol` (primary Java class, if available)
- Keyword-based filtering ensures only order- and stock-related files are indexed.

**Output**

- `data/raw_index/repo_index.jsonl`

No semantic interpretation is performed at this stage.

---

### 4.2 Domain and Structural Inference Layer

**Corresponding script:**  
- `02_build_domain_map.py`  
- Design specification: `docs/02_domain_map.md`

**Purpose**

This layer introduces an explicit **semantic intermediate representation** called the *domain map*, preventing fragmented or biased downstream sampling.

**Core Structures**

- **boundaries**: architectural layer classification  
  (`controller`, `service`, `mapper`, `model`, `config`, `other`)
- **entities**: business entity candidates extracted from class names
- **operations**: business operation candidates extracted from method names
- **candidate_flows**: heuristic business process skeletons

**Mechanisms**

- Domain inference via keyword voting (`choose_domain`)
- Layer inference via path/annotation heuristics (`infer_boundary`)
- Entity and operation aggregation with confidence scores and evidence
- Flow skeleton construction without assuming exact call graphs

**Output**

- `data/extracted/domain_map.json`

All items explicitly record `evidence_chunks`.

---

### 4.3 Rule and Flow Abstraction Layer

**Corresponding script:**  
- `03_extract_rules_and_flows.py`

**Purpose**

This layer bridges the gap between *semantic structure* and *questionable knowledge units*.

**Responsibilities**

- Scan indexed chunks for rule-related signals (state checks, locking, idempotency, transactions, exceptions, concurrency).
- Aggregate chunk-level signals into higher-level **rules**.
- Convert candidate flow skeletons into stable **flow records**.
- Link flows with related rules based on shared evidence or domain affinity.

**Outputs**

- `data/extracted/rules.jsonl`
- `data/extracted/flows.jsonl`

Each rule and flow maintains grounding through `evidence_chunks`.

---

### 4.4 Task-Oriented Sample Generation Layer

This layer materializes the abstract structures into concrete training tasks.

#### 4.4.1 QA Sample Generation

**Corresponding script:**  
- `04_generate_qa.py`

**Responsibilities**

- Generate rule-based QA samples.
- Generate flow-based QA samples.
- Support multilingual generation via templates.
- Attach:
  - evidence excerpts,
  - explicit reasoning traces,
  - task metadata.

**Outputs**

- `data/dataset/train.jsonl`
- `data/dataset/dev.jsonl`
- `data/dataset/test.jsonl`
- `data/samples/qa_samples.jsonl`

---

#### 4.4.2 Design Task Generation

**Corresponding script:**  
- `05_generate_design_tasks.py`

**Purpose**

This stage addresses *Scenario 2* of the assignment:  
deriving architecture-level design proposals grounded in an existing codebase.

**Responsibilities**

- Transform abstract requirements into structured design outputs.
- Enforce reuse of existing Controller/Service/Mapper layering.
- Explicitly reason about:
  - idempotency,
  - concurrency,
  - transactions,
  - compensations.

**Outputs**

- `data/dataset/design_train.jsonl`
- `data/samples/design_samples.jsonl`

---

#### 4.4.3 Multilingual Support

Multilingual capability is intentionally introduced at the task generation stage rather than during code analysis.

All upstream stages (indexing, domain mapping, rule and flow abstraction) are language-agnostic and operate purely on code structure and semantics.  
During QA and design task generation, language-specific templates are applied to the same underlying semantic units, allowing the same rule or flow to be expressed in different natural languages (e.g., Chinese or English).

A dedicated `language` field is maintained in each sample, and downstream SFT preparation supports language-based filtering, enabling flexible single-language or multilingual model training without duplicating semantic extraction.

---

### 4.5 Validation and Quality Control Layer

**Corresponding script:**  
- `06_postprocess_and_validate.py`

**Purpose**

Guarantee dataset correctness, consistency, and auditability.

**Validation Checks**

- Schema completeness for QA and Design samples.
- Evidence validity:
  - referenced `chunk_id`s must exist in `repo_index`.
- Minimum reasoning trace length.
- Instance-level deduplication.
- Coverage and quality statistics computation.

**Outputs**

- `data/dataset/stats.json`

---

### 4.6 Metrics and Reporting Layer

**Corresponding script:**  
- `07_metrics_report.py`

**Purpose**

Convert machine-readable statistics into human-readable results.

**Output**

- `docs/05_demo_and_results.md`

This file serves as a compact demonstration of dataset scale, diversity, and quality.

---

### 4.7 Model Adaptation and Inference Layer

Although not strictly required by the assignment, this layer validates the practical value of the generated data.

#### 4.7.1 SFT Data Preparation

**Corresponding script:**  
- `08_prepare_sft_data.py`

Transforms QA samples into instruction–response format suitable for supervised fine-tuning.

---

#### 4.7.2 LoRA Fine-tuning

**Corresponding script:**  
- `09_lora_finetune.py`

- Base model: Qwen2.5
- Lightweight LoRA adaptation
- Fully reproducible hyperparameters via environment variables

---

#### 4.7.3 Inference and Demo

**Corresponding script:**  
- `10_infer_lora.py`

Validates whether the adapted model can correctly answer repository-grounded business questions.

---

## 5. Traceability and Auditability Guarantees

Traceability is enforced end-to-end:

- `chunk_id`s originate in Step01 and are immutable.
- All higher-level structures reference these IDs.
- QA and Design samples embed evidence and reasoning traces.
- Validation explicitly checks grounding correctness.

Any sample can be traced back to exact source code lines.

---

## 6. Trade-offs and Design Decisions

### Intentional Trade-offs

- Keyword-based domain inference instead of full semantic parsing.
- Regex-based Java analysis instead of AST-level analysis.
- Conceptual flow skeletons instead of exact call graphs.

These choices favor explainability, controllability, and engineering feasibility.

---

## 7. Limitations and Extension Points

**Limitations**

- Naming-dependent heuristics may miss unconventional implementations.
- Flow skeletons do not guarantee execution order correctness.

**Extensions**

- Introduce finer-grained scenario templates (retry, timeout, idempotency).
- Add lightweight intra-file call edges.
- Extend to additional domains beyond order and stock.
- Incorporate feedback from model inference into data regeneration.

---

## 8. Summary

This project demonstrates how a local code repository can be transformed into
a structured, auditable training dataset through a layered, domain-aware pipeline.

By connecting repository indexing, semantic abstraction, task-oriented data generation,
and model adaptation in a single system, the pipeline provides a practical and extensible
solution for repository-grounded language model customization.













































## Multilingual Extension (Optional)

The proposed AutoCodeDataPipeline is language-agnostic by design.

All language-dependent components are isolated in the template layer, including:
- Question/answer templates for QA generation
- Requirement templates for design task generation
- Natural language descriptions in reasoning traces

Core stages such as:
- repository indexing and chunking,
- domain mapping,
- rule and flow extraction,
- evidence grounding,
- trace construction,
remain entirely independent of natural language.

Therefore, by switching or extending the template files (e.g., to English or bilingual templates),
the same pipeline can be reused to generate multilingual training datasets without modifying
any core extraction or validation logic.

This design allows the pipeline to be easily adapted for multilingual instruction tuning
or cross-lingual code understanding tasks.
